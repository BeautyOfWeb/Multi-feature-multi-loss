{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 30.0)\n",
    "#plt.ioff()\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.autograd import Variable\n",
    "    import torch.optim\n",
    "    \n",
    "    from dl.utils.solver import Solver\n",
    "    from dl.utils.utils import dist\n",
    "    from dl.models.dense_factor_conv import *\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if os.path.exists('C:/Users/Tianle/Documents/cs231n/spring1617/my-scripts'):\n",
    "    sys.path.append('C:/Users/Tianle/Documents/cs231n/spring1617/my-scripts')\n",
    "    root = 'C:/Users/Tianle/Documents/cs231n/spring1617/my-scripts/'\n",
    "if os.path.exists('/projects/academic/azhang/tianlema/deeplearning'):\n",
    "    sys.path.append('/projects/academic/azhang/tianlema/deeplearning')\n",
    "    root = '/projects/academic/azhang/tianlema/deeplearning/'\n",
    "from dl.utils.gen_conv_params import reduce_projections, get_itemset\n",
    "from dl.utils.sampler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unsqueeze(nn.Module):\n",
    "    \"\"\" Unsqueeze the second dimension by default\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=1):\n",
    "        super(Unsqueeze, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(self.dim)\n",
    "\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    \"\"\" Squeeze the last dimension by default\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=-1):\n",
    "        super(Squeeze, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(self.dim)\n",
    "\n",
    "    \n",
    "# Based on nn.Linear\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, num_layers=1):\n",
    "        super(Linear, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self, projections, out_features, in_channels, out_channels, kernel_size, \n",
    "                stride, threshold, force_square, bias, nonlinearity, use_batchnorm):\n",
    "        assert self.num_layers <= len(out_features)\n",
    "        assert out_channels == 1, ('This module is only used for compare (Dense)FactorBlock '\n",
    "                                   'model with out_channesl=1. Implement the same interface')\n",
    "        in_features = len(get_itemset(projections[0].keys(), projections[0]))\n",
    "        model = nn.Sequential()\n",
    "        for i in range(self.num_layers):\n",
    "            in_features = in_features if i==0 else out_feat\n",
    "            out_feat = out_features[i+len(out_features)-self.num_layers]\n",
    "            model.add_module('linear{0}'.format(i), nn.Linear(in_features, out_feat, bias=bias))\n",
    "            if use_batchnorm:\n",
    "                # In (Dense)FactorBlock we use nn.BatchNorm2d\n",
    "                model.add_module('batchnorm{0}'.format(i), nn.BatchNorm1d(out_feat))\n",
    "            # Should I add nonlinearity in the last layer?\n",
    "            # This is different from (Dense)FactorBlock \n",
    "            ##To do: add more layers after (Dense)FactorBlock\n",
    "            if i < self.num_layers - 1:\n",
    "                model.add_module('activation{0}'.format(i), nonlinearity)\n",
    "        # implement the interface for (Dense)FactorBlock whose output is (N, 1, d)\n",
    "        model.add_module('unsqueeze', Unsqueeze())\n",
    "        return model\n",
    "\n",
    "    \n",
    "# This will be used for comparison\n",
    "Linear1 = Linear(num_layers=1)\n",
    "Linear2 = Linear(num_layers=2)\n",
    "\n",
    "\n",
    "# Based on FactorConv\n",
    "# Do not use domain knowledge. This will increase a significant number of parameters\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, num_layers=1):\n",
    "        super(Conv, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self, projections, out_features, in_channels, out_channels, kernel_size, \n",
    "                stride, threshold, force_square, bias, nonlinearity, use_batchnorm):\n",
    "        assert self.num_layers <= len(out_features)\n",
    "        assert out_channels == 1, ('This module is only used for compare (Dense)FactorBlock '\n",
    "                                   'model with out_channesl=1. Implement the same interface')\n",
    "        in_features = len(get_itemset(projections[0].keys(), projections[0]))\n",
    "        # \"Destroy\" projection as if we don't know any factors, make it \"fully connected\"\n",
    "        projections = []\n",
    "        out_feats = out_features[(len(out_features)-self.num_layers):]\n",
    "        for i in range(self.num_layers):\n",
    "            in_features = in_features if i==0 else out_feats[i-1]\n",
    "            projections.append({i: list(range(in_features)) for i in range(out_feats[i])})\n",
    "        model = FactorBlock(projections, out_feats, in_channels, out_channels, kernel_size, \n",
    "                            stride, threshold, force_square, bias, nonlinearity, use_batchnorm)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "Conv1 = Conv(num_layers=1)\n",
    "Conv2 = Conv(num_layers=2)\n",
    "\n",
    "    \n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, out_channels, in_channels=1):\n",
    "        super(LinearEmbedding, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embed = nn.Conv1d(in_channels, out_channels, 1)\n",
    "    def forward(self, x):\n",
    "        # \"lazy\" input x of size (N, d)\n",
    "        if self.in_channels == 1:\n",
    "            if len(x.size()) == 1:\n",
    "                x = x.unsqueeze(1)\n",
    "            if len(x.size()) == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "        assert x.size(1) == self.in_channels\n",
    "        out = self.embed.forward(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    \"\"\" Used for categorical features\n",
    "    Args:\n",
    "        num_features: int, should be num_levels of feature\n",
    "        hidden_dim: int\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        assert num_features >= 2, '{0} < 2'.format(num_features)\n",
    "        self.W_embed = nn.Parameter(torch.Tensor(num_features, hidden_dim))\n",
    "        nn.init.normal(self.W_embed)\n",
    "        self.register_parameter('W_embed', self.W_embed)\n",
    "    def forward(self, x):\n",
    "        assert (x.dim() == 1 and isinstance(x.data, torch.LongTensor) \n",
    "                and min(x.data) >= 0 and max(x.data) <= self.num_features)\n",
    "        return self.W_embed[x]\n",
    "\n",
    "    \n",
    "class LossAvg(nn.Module):\n",
    "    \"\"\" Combine loss\n",
    "    Args:\n",
    "        num: how many losses to be combined\n",
    "    \"\"\"\n",
    "    def __init__(self, num):\n",
    "        super(LossAvg, self).__init__()\n",
    "        self.num = num\n",
    "        self.w = nn.Parameter(torch.Tensor(num))\n",
    "        nn.init.normal(self.w)\n",
    "        self.register_parameter('w', self.w)\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 1 and x.size(0) == self.num\n",
    "        tmp = self.w.exp()\n",
    "        return (x * tmp / tmp.sum()).sum()\n",
    "    \n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(1)\n",
    "\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self._reset()\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.val = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        self.avg = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "        \n",
    "def cal_acc(output, target, topk=(1,)):\n",
    "    target = target.contiguous().long()\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1)\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        acc = pred.eq(target.view(-1,1).expand(pred.size()))[:, :k].float().view(-1).sum(0)\n",
    "        acc.mul_(100 / target.size(0))\n",
    "        res.append(acc)\n",
    "    return res\n",
    "\n",
    "def binsort(n, bins, start=0):\n",
    "    assert sorted(bins) == bins\n",
    "    mid = len(bins) // 2\n",
    "    if len(bins) == 0:\n",
    "        return start\n",
    "    if n > bins[mid]:\n",
    "        start = start + mid + 1\n",
    "        return binsort(n, bins[(mid+1):], start)\n",
    "    else:\n",
    "        return binsort(n, bins[:mid], start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_TYPE='rnaseq'\n",
    "INPUT_SUFFIX='3rand'\n",
    "ALPHA=0.5\n",
    "TRAIN=1\n",
    "NORM_TYPE=0\n",
    "USE_ALL_DATA=True #if True, unbalanced; otherwise, balanced\n",
    "BATCH_SIZE=50\n",
    "NUM_ITER=100\n",
    "PRINT_EVERY=1\n",
    "MODEL=Linear1\n",
    "KERNEL_SIZE=10\n",
    "THRESHOLD=10\n",
    "WHICH_LOSS='ce'\n",
    "WEIGHT_DECAY=1e-3\n",
    "LOSS_INDEX=4\n",
    "SEED=1\n",
    "dim_age = 10\n",
    "dim_iss = 15\n",
    "num_partitions = int(INPUT_SUFFIX[0])\n",
    "try:\n",
    "    extra_partitions = int(INPUT_SUFFIX[-1])\n",
    "except ValueError:\n",
    "    extra_partitions = 0\n",
    "check_acc_idx = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if MODEL == FactorBlock:\n",
    "    model_name = 'fb'\n",
    "elif MODEL == DenseFactorBlock:\n",
    "    model_name = 'dense'\n",
    "elif MODEL == Linear1:\n",
    "    model_name = 'Linear1'\n",
    "elif MODEL == Linear2:\n",
    "    model_name = 'Linear2'\n",
    "elif MODEL == Conv1:\n",
    "    model_name = 'Conv1'\n",
    "elif MODEL == Conv2:\n",
    "    model_name = 'Conv2'\n",
    "else:\n",
    "    raise ValueError('MODEL {0} undefined'.format(MODEL.__repr__()))\n",
    "\n",
    "balance = 'unbal' if USE_ALL_DATA else 'bal'\n",
    "\n",
    "ckp_path = '../checkpoints/'\n",
    "script_file_prefix = ('{0}_{1}_seed{2}_norm{3}-{4}-train{5}-alpha{6}-reg{7}'.format(\n",
    "        EXP_TYPE, INPUT_SUFFIX, SEED, NORM_TYPE, model_name, TRAIN, ALPHA, math.log10(WEIGHT_DECAY)))\n",
    "ckp_file_prefix = ckp_path + script_file_prefix\n",
    "ckp_file = ckp_file_prefix + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path = root + 'mm-dream/processed_data/'\n",
    "fileName = 'exp_features_unnormalized.pkl'\n",
    "input_file = input_path + fileName\n",
    "with open(input_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "res = data['res']\n",
    "bins_age = [35, 50, 60, 70, 80] \n",
    "levels_age = len(bins_age) + 1\n",
    "levels_iss = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_input(data, exp_type=EXP_TYPE, norm_type=0, bins=bins_age, levels_iss=3):\n",
    "    x = data['exp_'+exp_type]\n",
    "    # how to normalize?\n",
    "    if norm_type == 1:\n",
    "        x = x / np.sum(x, axis=1, keepdims=True)\n",
    "    elif norm_type == 2:\n",
    "        x = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    elif norm_type == 3:\n",
    "        x = (x - np.mean(x, axis=1, keepdims=True)) / np.std(x, axis=1, keepdims=True)\n",
    "    clinical = data['clinical_'+exp_type]\n",
    "    var_names = data['var_names']\n",
    "    y_truth = np.array([[np.log2(float(v[var_names['D_PFS']])), np.log2(float(v[var_names['D_OS']])),\n",
    "                       float(v[var_names['D_OS_FLAG']]), float(v[var_names['D_PFS_FLAG']]),\n",
    "                       1 if v[var_names['HR_FLAG']] == 'TRUE' else 0] for v in clinical])\n",
    "    levels_age = len(bins) + 1\n",
    "    age = np.array([float(v[var_names['D_Age']]) for v in clinical if v[var_names['D_Age']] != 'NA'])\n",
    "    age = np.array([float(v[var_names['D_Age']]) \n",
    "                    if v[var_names['D_Age']] != 'NA' else np.mean(age) for v in clinical])  \n",
    "    age = np.array([binsort(a, bins) for a in age])\n",
    "\n",
    "    iss = np.array([int(v[var_names['D_ISS']]) - 1 for v in clinical if v[var_names['D_ISS']] != 'NA'])\n",
    "    tmp = np.bincount(iss)\n",
    "    tmp = tmp / np.sum(tmp)\n",
    "    iss = np.array([int(v[var_names['D_ISS']]) - 1 \n",
    "              if v[var_names['D_ISS']] != 'NA' else np.random.choice([0,1,2], p=tmp)\n",
    "              for v in clinical])\n",
    "\n",
    "    x_age = Variable(torch.from_numpy(age).float())\n",
    "    x_iss = Variable(torch.from_numpy(iss).float())\n",
    "    x_exp = Variable(torch.from_numpy(x).float())\n",
    "    y_truth = Variable(torch.from_numpy(y_truth).float())\n",
    "    return x_age, x_iss, x_exp, y_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_age, x_iss, x_exp, y_truth = prep_input(data, exp_type=EXP_TYPE, norm_type=NORM_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if INPUT_SUFFIX == '{0}rand'.format(num_partitions):\n",
    "    np.random.seed(SEED)\n",
    "    split = [e.tolist() for e in np.array_split(np.random.permutation(y_truth.size(0)), num_partitions)]\n",
    "elif INPUT_SUFFIX == '{0}rand{1}'.format(num_partitions, extra_partitions):\n",
    "    np.random.seed(SEED)\n",
    "    split = [e.tolist() for e in np.array_split(np.random.permutation(y_truth.size(0)), \n",
    "                                                num_partitions+extra_partitions)]\n",
    "    for i in range(1, 1+extra_partitions):\n",
    "        split[0] += split[i]\n",
    "    for i in range(extra_partitions):\n",
    "        del split[1]\n",
    "elif INPUT_SUFFIX == '{0}balanced'.format(num_partitions):\n",
    "    idx0 = np.where(y_truth[:,-1] == 0)[0]\n",
    "    idx0 = np.array_split(idx0, num_partitions)\n",
    "    idx1 = np.where(y_truth[:,-1] == 1)[0]\n",
    "    idx1 = np.array_split(idx1, num_partitions)\n",
    "    split = [np.concatenate([idx0[i], idx1[i]]).tolist() for i in range(num_partitions)]\n",
    "elif INPUT_SUFFIX == '3natural':\n",
    "    if EXP_TYPE == 'ma':\n",
    "        a = list(range(133))\n",
    "        b = list(range(133, 403))\n",
    "        c = list(range(403, 957))\n",
    "    elif EXP_TYPE == 'rnaseq':\n",
    "        a = list(range(142))\n",
    "        b = list(range(142, 283))\n",
    "        c = list(range(283, 424))\n",
    "    else:\n",
    "        raise ValueError('datatype should be either ma or rnaseq')\n",
    "    split = [a, b, c]\n",
    "else:\n",
    "        raise ValueError('split_type: {0} currently undefined'.format(INPUT_SUFFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for idx in split:\n",
    "    idx = torch.LongTensor(idx)\n",
    "    X.append([x_age[idx], x_iss[idx], x_exp[idx]])\n",
    "    Y.append(y_truth[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(X[0])\n",
    "num_losses = Y[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = TRAIN\n",
    "which_loss = WHICH_LOSS\n",
    "# solver params\n",
    "num_iter = NUM_ITER\n",
    "batch_size = BATCH_SIZE\n",
    "use_all_data = USE_ALL_DATA\n",
    "print_every = PRINT_EVERY\n",
    "lr = 1e-2\n",
    "weight_decay = WEIGHT_DECAY\n",
    "adj_lr_every = 1000\n",
    "lr_decay = 4e-1\n",
    "\n",
    "# model params\n",
    "threshold=THRESHOLD\n",
    "kernel_size=KERNEL_SIZE \n",
    "in_channels=1\n",
    "out_channels=1\n",
    "num_layers=2\n",
    "stride=2 \n",
    "force_square=False\n",
    "bias=False\n",
    "nonlinearity=nn.ReLU6() \n",
    "use_batchnorm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fns = [nn.SmoothL1Loss()]*2 + [nn.CrossEntropyLoss()]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "optimizers = []\n",
    "\n",
    "for out_dim in [1]*2 + [2]*3:\n",
    "    pathway_to_output = {k: list(range(len(res[0][num_layers-1]))) for k in range(out_dim)}\n",
    "    projections = [res[0][1], pathway_to_output]\n",
    "    \n",
    "    model_age = nn.Sequential(Embedding(levels_age, dim_age), nn.Linear(dim_age, out_dim))\n",
    "    optimizer_age = torch.optim.Adam(model_age.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model_iss = nn.Sequential(Embedding(levels_iss, dim_iss), nn.Linear(dim_iss, out_dim))\n",
    "    optimizer_iss = torch.optim.Adam(model_iss.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model_exp = MODEL(projections, out_features=[len(v) for v in projections], in_channels=in_channels, \n",
    "                        out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                        threshold=threshold, force_square=force_square, bias=bias,\n",
    "                        nonlinearity=nonlinearity, use_batchnorm=use_batchnorm)\n",
    "    optimizer_exp = torch.optim.Adam(model_exp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    models.append([model_age, model_iss, model_exp])\n",
    "    optimizers += [optimizer_age, optimizer_iss, optimizer_exp]\n",
    "    model_avg = nn.Sequential(nn.Conv1d(out_dim, out_dim, len(models[-1])), Squeeze(-1))\n",
    "    models[-1].append(model_avg)\n",
    "    optimizers.append(torch.optim.Adam(model_avg.parameters(), lr=lr, weight_decay=weight_decay))\n",
    "    \n",
    "model_loss_avg = []\n",
    "for i in range(num_losses):\n",
    "    # models.shape[0] = num_losses, models.shape[1] = num_features + 1 (the last one is for model_avg) \n",
    "    model_loss_avg.append(LossAvg(num_features))\n",
    "# The last one is for the final loss\n",
    "model_loss_avg.append(LossAvg(num_losses))\n",
    "[optimizers.append(torch.optim.Adam(m.parameters(), lr=lr, weight_decay=weight_decay)) \n",
    " for m in model_loss_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(ckp_file):\n",
    "    checkpoint = torch.load(ckp_file)\n",
    "    models = checkpoint['models']\n",
    "    model_loss_avg = checkpoint['model_loss_avg']\n",
    "    losses_history = checkpoint['losses_history']\n",
    "    losses_avg_history = checkpoint['losses_avg_history']\n",
    "    acc_history = checkpoint['acc_history']\n",
    "    max_min_acc = checkpoint['max_min_acc']\n",
    "    num_iter_done = len(losses_avg_history)\n",
    "else:\n",
    "    losses_history = []\n",
    "    losses_avg_history = []\n",
    "    acc_history = []\n",
    "    # maximize min(acc_train, acc_val, acc_test) during iteration\n",
    "    # [max_acc, iteration, (acc_train, acc_val, acc_test)]\n",
    "    max_min_acc = [0, 0, (0, 0, 0)]\n",
    "    num_iter_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(x, y_truth, model, loss_fn=nn.SmoothL1Loss(), check_acc=False, \n",
    "             use_all_data=True, batch_size=None):\n",
    "    if use_all_data:\n",
    "        idx = torch.LongTensor(range(x.size(0)))\n",
    "    else:\n",
    "        # NOT tested and not used\n",
    "        assert isinstance(batch_size, int)\n",
    "        idx = torch.LongTensor(\n",
    "            balanced_sampler(y_truth.data.long(), batch_size=batch_size, num_iter=1)[0])\n",
    "    y = model(x[idx])\n",
    "    if y.dim() == 3 and y.size(1) == 1:\n",
    "        # y.size() = (N, 1, d) as out_channels=1 for model_exp\n",
    "        y = y.squeeze(1)\n",
    "    assert y.dim() == 2, 'y.dim()={0}'.format(y.dim())\n",
    "    # For classification tasks, output is N x C\n",
    "    if y.size(1) > 1:\n",
    "        y_truth = y_truth.long()\n",
    "    loss = loss_fn(y, y_truth[idx])\n",
    "    if check_acc:\n",
    "        acc = cal_acc(y, y_truth[idx])[0].data[0]\n",
    "    else:\n",
    "        acc = None\n",
    "    return loss, y, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_one_iter(x, y, models, model_loss_avg, loss_fns, embedding_idx=[0,1], \n",
    "                  check_acc_idx=[4], alpha=ALPHA):\n",
    "    \"\"\" eval the whole model with data (x, y)\n",
    "    Args:\n",
    "        x: Variable\n",
    "        y: Variable\n",
    "        models: num_losses * (num_features + 1) models\n",
    "        model_loss_avg: num_losses + 1 models, the last one is for the final node\n",
    "        loss_fns: a list of loss functions; len(loss_fns) = num_losses\n",
    "        check_acc_idx: acc will be calculated for these indices\n",
    "        alpha: float number in [0, 1]; y_pred = alpha*y_pred_avg + (1-alpha)*y_pred_conv\n",
    "    Returns:\n",
    "        loss: Variable\n",
    "        loss_history: two-dimensional list: num_losses * (num_features + 2)\n",
    "        loss_avg_history: one-dimensional list: num_losses + 1 (final loss)\n",
    "        acc_history: one-dimensional list: len(check_acc_idx) * (num_features + 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    num_losses = len(models)\n",
    "    num_features = len(models[0]) - 1 # models[][-1] is nn.Conv1d layer\n",
    "    assert len(model_loss_avg) == num_losses + 1 # models_loss_avg[-1] is for the final loss\n",
    "    assert len(loss_fns) == num_losses\n",
    "    assert isinstance(check_acc_idx, (list, tuple))\n",
    "    assert alpha >= 0 and alpha <= 1\n",
    "    \n",
    "    loss_all = []\n",
    "    for idx_loss in range(num_losses):\n",
    "        loss_feature = []\n",
    "        y_pred_feature = []\n",
    "        check_acc = (idx_loss in check_acc_idx)\n",
    "        for idx_feature in range(num_features):\n",
    "            x_tmp = x[idx_feature] \n",
    "            if idx_feature in embedding_idx:\n",
    "                x_tmp = x[idx_feature].long()\n",
    "            loss, y_pred, acc = get_loss(x_tmp, y[:, idx_loss], \n",
    "                                         models[idx_loss][idx_feature],\n",
    "                                         loss_fn=loss_fns[idx_loss], \n",
    "                                         check_acc=check_acc)\n",
    "            \n",
    "            if check_acc:\n",
    "                acc_history.append(acc)\n",
    "            loss_feature.append(loss)\n",
    "            y_pred_feature.append(y_pred)\n",
    "        # loss_avg is the weighted loss of loss_feature\n",
    "        loss_avg = model_loss_avg[idx_loss](torch.cat(loss_feature))\n",
    "        # After the next line, y_pred_feature will be: N * out_dim * num_features\n",
    "        y_pred_feature = torch.stack(y_pred_feature, -1)\n",
    "        loss_conv, y_pred_conv, acc = get_loss(y_pred_feature, y[:, idx_loss], \n",
    "                                               models[idx_loss][num_features],\n",
    "                                               loss_fn=loss_fns[idx_loss], \n",
    "                                               check_acc=check_acc)\n",
    "       \n",
    "        w_tmp = model_loss_avg[idx_loss].w.exp()\n",
    "        w_tmp = w_tmp/w_tmp.sum()\n",
    "        # y_pred_avg: weighted sum; after next line, it will be: N * out_dim\n",
    "        y_pred_avg = (y_pred_feature * w_tmp).sum(-1)   \n",
    "        # y_pred: weighted mean as final prediction\n",
    "        y_pred = alpha*y_pred_avg + (1-alpha)*y_pred_conv\n",
    "        if check_acc:\n",
    "            # +=[acc_conv, acc_avg, acc_combined]\n",
    "            acc_history = acc_history + [acc, \n",
    "                                         cal_acc(y_pred_avg, y[:, idx_loss])[0].data[0], \n",
    "                                         cal_acc(y_pred, y[:, idx_loss])[0].data[0]]\n",
    "        # combine loss from features\n",
    "        loss_all.append(alpha*loss_avg + (1-alpha)*loss_conv)\n",
    "        loss_history.append([loss.data[0] for loss in loss_feature] + \n",
    "                            [loss_avg.data[0], loss_conv.data[0]])\n",
    "    # final loss\n",
    "    loss = model_loss_avg[-1](torch.cat(loss_all))\n",
    "    loss_avg_history = [loss.data[0] for loss in loss_all]\n",
    "    loss_avg_history.append(loss.data[0])\n",
    "    # add y_pred for mm-dream submission\n",
    "    return loss, loss_history, loss_avg_history, acc_history, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_acc_repr = []\n",
    "for exp_type in ['rnaseq', 'ma']:\n",
    "    x_age, x_iss, x_exp, y_truth = prep_input(data, exp_type, NORM_TYPE)\n",
    "    _, _, _, acc, y_pred = eval_one_iter([x_age, x_iss, x_exp], y_truth, models, model_loss_avg, \n",
    "                             loss_fns, check_acc_idx=check_acc_idx)\n",
    "    acc_repr = '{0}_acc={1}; '.format(exp_type, acc)\n",
    "    init_acc_repr.append(acc_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train partition {0}, start at iteration {1}, end at {2}'\n",
    "      .format(train, num_iter_done, num_iter_done + num_iter))\n",
    "\n",
    "def zero_grad(optimizer, i, adj_lr_every, lr_decay):   \n",
    "    if (i + 1) % adj_lr_every == 0:\n",
    "        for i in range(len(optimizer.param_groups)):\n",
    "            optimizer.param_groups[i]['lr'] = optimizer.param_groups[i]['lr'] * lr_decay\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "for i in range(num_iter_done, num_iter_done + num_iter):\n",
    "    res_loss = [eval_one_iter(x, y, models, model_loss_avg, loss_fns, check_acc_idx=check_acc_idx) \n",
    "                for x, y in zip(X, Y)]\n",
    "    # len(X) = len(Y) = len(Acc) = num_partitions\n",
    "    # = len(losses) = len(loss_history) = len(loss_avg_history) + len(acc)\n",
    "    losses, loss_history, loss_avg_history, acc, _ = [[v[i] for v in res_loss] \n",
    "                                                   for i in range(len(res_loss[0]))]\n",
    "    loss = losses[train]\n",
    "    losses_history.append(loss_history)\n",
    "    losses_avg_history.append(loss_avg_history)\n",
    "    acc_history.append(acc)\n",
    "    acc_current = [a[-1] for a in acc] \n",
    "    if i == num_iter_done:\n",
    "        initial_acc = acc_current\n",
    "    if min(acc_current) > max_min_acc[0]:\n",
    "        max_min_acc = [min(acc_current), i, acc_current]\n",
    "        print('{0}: New max_min_acc={1}'.format(i+1, max_min_acc))\n",
    "        model_states = {'models': [[m.state_dict() for m in v] for v in models], \n",
    "                       'model_loss_avg': [m.state_dict() for m in model_loss_avg]}\n",
    "        checkpoint = {'model_states': model_states,\n",
    "                      'models': models,\n",
    "                      'model_loss_avg': model_loss_avg,\n",
    "                      'losses_history': losses_history,\n",
    "                      'losses_avg_history': losses_avg_history,\n",
    "                      'acc_history': acc_history,\n",
    "                      'max_min_acc': max_min_acc}\n",
    "        torch.save(checkpoint, ckp_file) \n",
    "        \n",
    "    [zero_grad(optimizer, i, adj_lr_every, lr_decay) for optimizer in optimizers]\n",
    "    loss.backward(retain_graph=True)\n",
    "    [optimizer.step() for optimizer in optimizers]\n",
    "    \n",
    "    if (i + 1) % print_every == 0:\n",
    "        print('{0}: acc={1}, acc.avg={2}, acc.conv={3}'.format(i+1, [a[-1] for a in acc],\n",
    "                                                               [a[-2] for a in acc], \n",
    "                                                               [a[-3] for a in acc]))\n",
    "        # loss_history is NOT losses_history\n",
    "        print('{0}: hr_risk losses = {1}'.format(i+1, [v[-1] for v in loss_history]))\n",
    "        print('{0}: final loss = {1}'.format(i+1, [v[-1] for v in loss_avg_history]))\n",
    "        sys.stdout.flush() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_states = {'models': [[m.state_dict() for m in v] for v in models], \n",
    "                'model_loss_avg': [m.state_dict() for m in model_loss_avg]}\n",
    "checkpoint = {'model_states': model_states,\n",
    "              'models': models,\n",
    "              'model_loss_avg': model_loss_avg,\n",
    "              'losses_history': losses_history,\n",
    "              'losses_avg_history': losses_avg_history,\n",
    "              'acc_history': acc_history,\n",
    "              'max_min_acc': max_min_acc}\n",
    "ckp_file2 = ('{0}-n{1}.pt'.format(ckp_file_prefix, num_iter_done+num_iter))\n",
    "torch.save(checkpoint, ckp_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(ckp_file):\n",
    "    checkpoint = torch.load(ckp_file)\n",
    "    models = checkpoint['models']\n",
    "    model_loss_avg = checkpoint['model_loss_avg']\n",
    "    \n",
    "    ckp_file = '{0}_{1}_seed{2}_models.pt'.format(EXP_TYPE, INPUT_SUFFIX, SEED)\n",
    "    torch.save({'models': models, 'model_loss_avg': model_loss_avg, \n",
    "                'entrezIds': res[1][1]}, ckp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_acc_repr = []\n",
    "for exp_type in ['rnaseq', 'ma']:\n",
    "    x_age, x_iss, x_exp, y_truth = prep_input(data, exp_type, NORM_TYPE)\n",
    "    _, _, _, acc, y_pred = eval_one_iter([x_age, x_iss, x_exp], y_truth, models, model_loss_avg, \n",
    "                             loss_fns, check_acc_idx=check_acc_idx)\n",
    "    acc_repr = '{0}_acc={1}; '.format(exp_type, acc)\n",
    "    final_acc_repr.append(acc_repr)\n",
    "    print('{0}: y_pred.size()={1}'.format(exp_type, y_pred.size()))\n",
    "print('Before optimization: {0}\\n{1}'.format(*init_acc_repr))\n",
    "print('After optimization: {0}\\n{1}'.format(*final_acc_repr))\n",
    "\n",
    "print('Initial acc =', initial_acc)\n",
    "print('Final max_min_acc =', max_min_acc)\n",
    "start = num_iter_done\n",
    "end = num_iter_done + num_iter\n",
    "markers = ['ro', 'b+', 'g^']\n",
    "plt.subplot(311)\n",
    "plt.title(' max_min_acc={0}\\n initial_acc={1}'.format(max_min_acc, initial_acc))\n",
    "plt.ylabel('loss_final')\n",
    "for idx, marker in zip(range(num_partitions), markers):\n",
    "    plt.plot([v[idx][-1] for v in losses_avg_history][start:end], marker)\n",
    "plt.subplot(312)\n",
    "plt.title('Initial acc:{0}\\n{1}'.format(*init_acc_repr))\n",
    "plt.ylabel('loss_hr_risk')\n",
    "for idx, marker in zip(range(num_partitions), markers):\n",
    "    plt.plot([v[idx][-2] for v in losses_avg_history][start:end], marker)\n",
    "plt.subplot(313)\n",
    "plt.title('Final acc:{0}\\n{1}'.format(*final_acc_repr))\n",
    "plt.ylabel('acc_hr_risk')\n",
    "for idx, marker in zip(range(num_partitions), markers):\n",
    "    plt.plot([v[idx][-1] for v in acc_history][start:end], marker)\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
